{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST without Acquisition function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on [this github](https://github.com/jiuntian/pytorch-mnist-example/blob/master/pytorch-mnist.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 512\n",
    "batch_size_test = 1024 #\n",
    "\n",
    "# define how image transformed\n",
    "image_transform = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])\n",
    "#image datasets\n",
    "train_dataset = torchvision.datasets.MNIST('dataset/', \n",
    "                                           train=True, \n",
    "                                           download=True,\n",
    "                                           transform=image_transform)\n",
    "test_dataset = torchvision.datasets.MNIST('dataset/', \n",
    "                                          train=False, \n",
    "                                          download=True,\n",
    "                                          transform=image_transform)\n",
    "#data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size_train, \n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size_test, \n",
    "                                          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import matplotlib.pyplot as plt\n",
    "# We can check the dataloader\n",
    "_, (example_datas, labels) = next(enumerate(test_loader))\n",
    "sample = example_datas[0][0]\n",
    "# show the data\n",
    "plt.imshow(sample, cmap='gray', interpolation='none')\n",
    "print(\"Label: \"+ str(labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initializing the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5, stride=1)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.conv2_drop_for_unc = nn.Dropout2d(p=0.3)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.dropout = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2_drop(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = x.view(-1, 320)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x)\n",
    "        # for the dropout based uncertainty method\n",
    "        x = self.custom_dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "    def custom_dropout(self, x):\n",
    "        if self.dropout:\n",
    "            # Apply dropout during training\n",
    "            return F.dropout(x, p=0.3)\n",
    "        else:\n",
    "            # No dropout during evaluation\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model and optimizer\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "##define train function\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval=10000):\n",
    "    model.train()\n",
    "    tk0 = tqdm(train_loader, total=int(len(train_loader)))\n",
    "    counter = 0\n",
    "    for batch_idx, (data, target) in enumerate(tk0):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        counter += 1\n",
    "        tk0.set_postfix(loss=(loss.item()*data.size(0) / (counter * train_loader.batch_size)))\n",
    "##define test function\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 2\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with acquisition function\n",
    "\n",
    "In this part we will train the CNN by using multiple different acquisition functions to see which acquisition function works best.\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "We will compare the different acquisition function by plotting the highest achieved accuracy compared to the number of datapoints requested by the acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(model, dataset, batch_size=1028):\n",
    "    model.eval()\n",
    "    entropies = []\n",
    "\n",
    "    # Create a DataLoader for the dataset with the specified batch size\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for inputs, _ in data_loader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs.to(device))\n",
    "            # Calculate softmax to get probabilities\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            # Calculate entropy for each sample in the batch\n",
    "            entropy = -torch.sum(probabilities * torch.log2(probabilities + 1e-10), dim=1)\n",
    "            # Append the entropies for the batch to the list\n",
    "            entropies.extend(entropy.cpu().numpy())\n",
    "\n",
    "    return np.array(entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_pool_entropy(model, pool, current_labeled_dataset, amount_of_labels):\n",
    "    entropies = entropy(model, train_dataset)\n",
    "\n",
    "    # Convert entropies to a numpy array for sorting\n",
    "    entropies = np.array(entropies)\n",
    "\n",
    "    # Sort the dataset based on entropy values (in ascending order)\n",
    "    sorted_indices = np.argsort(entropies)\n",
    "\n",
    "    # Get the indices of the \"amount_of_labels\" data points with the highest entropy values\n",
    "    top_entropy_indices = sorted_indices[-amount_of_labels:]\n",
    "\n",
    "    # Create a new dataset with the \"amount_of_labels\" data points with the highest entropy\n",
    "    high_entropy_data = [train_dataset[i] for i in top_entropy_indices]\n",
    "\n",
    "    # Remove the \"amount_of_labels\" high-entropy data points from the original dataset\n",
    "    for i in sorted(top_entropy_indices, reverse=True):\n",
    "        pool.data = torch.cat((pool.data[:i], pool.data[i+1:]))\n",
    "        pool.targets = torch.cat((pool.targets[:i], pool.targets[i+1:]))\n",
    "\n",
    "    current_labeled_dataset.extend(high_entropy_data)\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning_loop_entropy(model, device, full_labeled_dataset, optimizer, target_accuracy=0.90, growth_rate=1000, N=5, batch_size_train=512):\n",
    "    active_learning_dataset = []  # Initialize the active learning dataset\n",
    "    current_epoch = 0\n",
    "    accuracies = dict()\n",
    "    label_counts_per_epoch = dict()\n",
    "\n",
    "    while True:\n",
    "        # Train on the current active learning dataset for N epochs\n",
    "        print(f\"getting {growth_rate} number of datapoints 'labeled'\")\n",
    "        get_labels_from_pool_entropy(model, full_labeled_dataset, active_learning_dataset, growth_rate)\n",
    "        label_counts = Counter([label for _, label in active_learning_dataset])\n",
    "        label_counts_per_epoch[current_epoch] = dict()\n",
    "        for label, counts in label_counts.items():\n",
    "            label_counts_per_epoch[current_epoch][label] = counts\n",
    "\n",
    "        print(f\"Training on current dataset for {N} epochs on {len(active_learning_dataset)} datapoints\")\n",
    "        active_learning_loader = torch.utils.data.DataLoader(active_learning_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "        for epoch in range(current_epoch, current_epoch + N):\n",
    "            train(model, device, active_learning_loader, optimizer, epoch)  # Use active_learning_loader for training\n",
    "            accuracy = test(model, device, test_loader)\n",
    "            if len(active_learning_dataset) not in accuracies:\n",
    "                accuracies[len(active_learning_dataset)] = []\n",
    "\n",
    "            accuracies[len(active_learning_dataset)].append(accuracy)\n",
    "            \n",
    "            current_epoch += 1\n",
    "\n",
    "        if accuracy >= target_accuracy:\n",
    "            print(f\"after {current_epoch} epochs and {len(active_learning_dataset)} number of datapoints an accuracy of {target_accuracy} is reached\")\n",
    "            break\n",
    "\n",
    "    return accuracies, label_counts_per_epoch\n",
    "        \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST('dataset/', \n",
    "                                           train=True, \n",
    "                                           download=True,\n",
    "                                           transform=image_transform)\n",
    "\n",
    "model1 = CNN().to(device)\n",
    "optimizer = optim.SGD(model1.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "nrpooledvalues_acc_entropy, label_counts_per_epoch_entropy = active_learning_loop_entropy(model1, device, train_dataset, optimizer)\n",
    "\n",
    "filename = 'nrpooledvalues_acc_entropy.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(nrpooledvalues_acc_entropy, file)\n",
    "\n",
    "filename = 'label_counts_per_epoch_entropy.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(label_counts_per_epoch_entropy, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dropout difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_diff(model, dataset, batch_size=1028):\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "    differences = []\n",
    "\n",
    "    # Create a DataLoader for the dataset with the specified batch size\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for inputs, _ in data_loader:\n",
    "        with torch.no_grad():\n",
    "            model.dropout = True\n",
    "            outputs_wdropout = model(inputs.to(device))\n",
    "            model.dropout = False\n",
    "            outputs_normal = model(inputs.to(device))\n",
    "\n",
    "            loss = criterion(outputs_wdropout, outputs_normal)\n",
    "\n",
    "            loss_per_item = torch.mean(loss, dim=1, keepdim=True)\n",
    "\n",
    "            # Append the entropies for the batch to the list\n",
    "            differences.extend(loss_per_item.cpu().numpy())\n",
    "\n",
    "\n",
    "    return np.array(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_dropout_diff(model, pool, current_labeled_dataset, amount_of_labels):\n",
    "    dropout_diffs = dropout_diff(model, train_dataset)\n",
    "\n",
    "    # Convert entropies to a numpy array for sorting\n",
    "    dropout_diffs = np.array(dropout_diffs)\n",
    "    dropout_diffs = np.concatenate(dropout_diffs, axis=0)\n",
    "\n",
    "    # Sort the dataset based on entropy values (in ascending order)\n",
    "    sorted_indices = np.argsort(dropout_diffs)\n",
    "\n",
    "    # Get the indices of the \"amount_of_labels\" data points with the highest entropy values\n",
    "    top_entropy_indices = sorted_indices[-amount_of_labels:]\n",
    "\n",
    "    # Create a new dataset with the \"amount_of_labels\" data points with the highest entropy\n",
    "    high_diff_data = [train_dataset[i] for i in top_entropy_indices]\n",
    "\n",
    "    # Remove the \"amount_of_labels\" high-entropy data points from the original dataset\n",
    "    for i in sorted(top_entropy_indices, reverse=True):\n",
    "        pool.data = torch.cat((pool.data[:i], pool.data[i+1:]))\n",
    "        pool.targets = torch.cat((pool.targets[:i], pool.targets[i+1:]))\n",
    "\n",
    "    current_labeled_dataset.extend(high_diff_data)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning_loop_dropout_diff(model, device, full_labeled_dataset, optimizer, target_accuracy=0.90, growth_rate=1000, N=5, batch_size_train=512):\n",
    "    active_learning_dataset = []  # Initialize the active learning dataset\n",
    "    current_epoch = 0\n",
    "    accuracies = dict()\n",
    "    label_counts_per_epoch = dict()\n",
    "\n",
    "    while True:\n",
    "        # Train on the current active learning dataset for N epochs\n",
    "        print(f\"getting {growth_rate} number of datapoints 'labeled'\")\n",
    "        get_labels_from_dropout_diff(model, full_labeled_dataset, active_learning_dataset, growth_rate)\n",
    "        label_counts = Counter([label for _, label in active_learning_dataset])\n",
    "        label_counts_per_epoch[current_epoch] = dict()\n",
    "        for label, counts in label_counts.items():\n",
    "            label_counts_per_epoch[current_epoch][label] = counts\n",
    "\n",
    "        print(f\"Training on current dataset for {N} epochs on {len(active_learning_dataset)} datapoints\")\n",
    "        active_learning_loader = torch.utils.data.DataLoader(active_learning_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "        print(model.dropout)\n",
    "        for epoch in range(current_epoch, current_epoch + N):\n",
    "            train(model, device, active_learning_loader, optimizer, epoch)  # Use active_learning_loader for training\n",
    "            accuracy = test(model, device, test_loader)\n",
    "            if len(active_learning_dataset) not in accuracies:\n",
    "                accuracies[len(active_learning_dataset)] = []\n",
    "\n",
    "            accuracies[len(active_learning_dataset)].append(accuracy)\n",
    "\n",
    "            current_epoch += 1\n",
    "\n",
    "        if accuracy >= target_accuracy:\n",
    "            print(f\"after {current_epoch} epochs and {len(active_learning_dataset)} number of datapoints an accuracy of {target_accuracy} is reached\")\n",
    "            break\n",
    "\n",
    "    return accuracies, label_counts_per_epoch\n",
    "        \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST('dataset/', \n",
    "                                           train=True, \n",
    "                                           download=True,\n",
    "                                           transform=image_transform)\n",
    "\n",
    "model2 = CNN().to(device)\n",
    "optimizer = optim.SGD(model2.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "nrpooledvalues_acc_dropout_diff, label_counts_per_epoch_dropout_diff = active_learning_loop_dropout_diff(model2, device, train_dataset, optimizer)\n",
    "\n",
    "filename = 'nrpooledvalues_acc_dropout_diff.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(nrpooledvalues_acc_dropout_diff, file)\n",
    "\n",
    "filename = 'label_counts_per_epoch_dropout_diff.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(label_counts_per_epoch_dropout_diff, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_pool_random(model, pool, current_labeled_dataset, amount_of_labels):\n",
    "    indexes = random.sample(range(len(pool)), amount_of_labels)\n",
    "    data_points = [train_dataset[i] for i in indexes]\n",
    "\n",
    "    # Remove the \"amount_of_labels\" high-entropy data points from the original dataset\n",
    "    for i in sorted(indexes, reverse=True):\n",
    "        pool.data = torch.cat((pool.data[:i], pool.data[i+1:]))\n",
    "        pool.targets = torch.cat((pool.targets[:i], pool.targets[i+1:]))\n",
    "\n",
    "    current_labeled_dataset.extend(data_points)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_learning_loop(model, device, full_labeled_dataset, optimizer, target_accuracy=0.90, growth_rate=1000, N=5, batch_size_train=512):\n",
    "    active_learning_dataset = []  # Initialize the active learning dataset\n",
    "    current_epoch = 0\n",
    "    accuracies = dict()\n",
    "\n",
    "    while True:\n",
    "        # Train on the current active learning dataset for N epochs\n",
    "        print(f\"getting {growth_rate} number of datapoints 'labeled'\")\n",
    "        get_labels_from_pool_random(model, full_labeled_dataset, active_learning_dataset, growth_rate)\n",
    "\n",
    "        print(f\"Training on current dataset for {N} epochs on {len(active_learning_dataset)} datapoints\")\n",
    "        active_learning_loader = torch.utils.data.DataLoader(active_learning_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "        for epoch in range(current_epoch, current_epoch + N):\n",
    "            train(model, device, active_learning_loader, optimizer, epoch)  # Use active_learning_loader for training\n",
    "            accuracy = test(model, device, test_loader)\n",
    "            if len(active_learning_dataset) not in accuracies:\n",
    "                accuracies[len(active_learning_dataset)] = []\n",
    "\n",
    "            accuracies[len(active_learning_dataset)].append(accuracy)\n",
    "            \n",
    "            current_epoch += 1\n",
    "\n",
    "        if accuracy >= target_accuracy:\n",
    "            print(f\"after {current_epoch} epochs and {len(active_learning_dataset)} number of datapoints an accuracy of {target_accuracy} is reached\")\n",
    "            break\n",
    "\n",
    "    return accuracies\n",
    "        \n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrpooledvalues_acc_random = random_learning_loop(model, device, train_dataset, optimizer)\n",
    "\n",
    "filename = 'nrpooledvalues_acc_random.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(nrpooledvalues_acc_random, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HITL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
